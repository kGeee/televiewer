# Phase 05, Plan 03: Migration Script & Cleanup

---

## Plan Metadata

```yaml
phase: 5
plan: 05-03
wave: 3
depends_on: ['05-01', '05-02']
autonomous: true
files_modified:
  - scripts/migrate-to-parquet.ts
  - src/lib/server/db/schema.ts
  - src/lib/server/db/migrations/ (optional)
  - package.json (scripts)
estimated_context: 20%
```

## must_haves

```yaml
truths:
  - 'Existing sessions with JSON telemetry are migrated to Parquet format'
  - 'Migration script converts all sessions without Parquet files'
  - '50%+ compression achieved: Parquet files smaller than equivalent JSON arrays'
  - 'Old lap_telemetry rows can be archived/truncated after verification'

artifacts:
  - path: 'scripts/migrate-to-parquet.ts'
    provides: 'One-time migration script for existing sessions'
    min_lines: 150
  - path: 'src/lib/server/telemetry/migration.ts'
    provides: 'Migration utilities (session conversion functions)'
    min_lines: 80
  - path: 'data/telemetry/*.parquet'
    provides: 'Migrated telemetry files for all existing sessions'
    min_lines: 0

key_links:
  - from: 'scripts/migrate-to-parquet.ts'
    to: 'src/lib/server/db/client.ts'
    via: 'DB query for existing lap_telemetry data'
  - from: 'scripts/migrate-to-parquet.ts'
    to: 'src/lib/server/telemetry/parquet.ts:writeSessionTelemetry()'
    via: 'Writes converted data to Parquet'
  - from: 'DB lap_telemetry table'
    to: 'Parquet files'
    via: 'Migration script row-by-row conversion'
```

---

## Objective

Create and run a one-time migration script that converts all existing telemetry data from PostgreSQL JSON arrays to Parquet files. Verify compression targets are met, and establish a path for cleaning up the old database storage after successful migration.

---

## Task 1: Create Migration Utilities Module

**Type:** auto

**Files:** `src/lib/server/telemetry/migration.ts`

**Action:**
Create a utility module with functions to convert existing database telemetry to the Parquet format. This module will be used by both the migration script and any future data handling needs.

**Functions to Implement:**

```typescript
import type { SessionTelemetry, LapTelemetry } from './types';

// Convert DB row format to SessionTelemetry structure
export function convertDbTelemetryToParquetFormat(
	sessionId: number,
	dbTelemetry: Array<{
		lapNumber: number;
		time: number[] | null;
		distance: number[] | null;
		lat: number[] | null;
		long: number[] | null;
		speed: number[] | null;
		rpm: number[] | null;
		throttle: number[] | null;
		brake: number[] | null;
		gear: number[] | null;
		steering: number[] | null;
		glat?: number[] | null;
		glong?: number[] | null;
	}>,
	auxChannels: Array<{
		lapId?: number;
		lapNumber?: number; // Need to join to get this
		name: string;
		data: number[];
	}>
): SessionTelemetry;

// Build points from columnar arrays
export function buildPointsFromColumnarData(
	columns: Record<string, number[] | null>,
	rowCount: number
): Array<Record<string, number>>;

// Get list of sessions needing migration
export async function getSessionsNeedingMigration(): Promise<
	Array<{
		sessionId: number;
		hasParquet: boolean;
		lapCount: number;
		hasDbData: boolean;
	}>
>;

// Validate migration (compare row counts)
export async function validateMigration(sessionId: number): Promise<{
	success: boolean;
	dbRowCount: number;
	parquetRowCount: number;
	missingLaps: number[];
}>;
```

**Implementation Notes:**

1. Handle null arrays gracefully (treat as empty or skip)
2. Ensure auxiliary channels are properly linked to their laps
3. The conversion must preserve all data points exactly
4. Track source metadata (original import type from telemetry_sources table)

**Verify:**

```bash
npx tsc --noEmit src/lib/server/telemetry/migration.ts 2>&1 | head -10
```

**Done:**

- [ ] All conversion functions implemented
- [ ] TypeScript compiles without errors
- [ ] Functions handle edge cases (null arrays, missing data)

---

## Task 2: Create Migration Script

**Type:** auto

**Files:** `scripts/migrate-to-parquet.ts`

**Action:**
Create a standalone migration script that can be run via `npx tsx` to convert all existing sessions to Parquet format.

**Script Requirements:**

1. **Dry-run mode**: Show what would be migrated without making changes
2. **Batch processing**: Migrate sessions in batches (e.g., 10 at a time)
3. **Progress reporting**: Show progress bar or count during migration
4. **Error handling**: Continue on individual session errors, report at end
5. **Validation**: After migration, verify row counts match

**Script Structure:**

```typescript
#!/usr/bin/env tsx
import { db } from '../src/lib/server/db/client';
import {
	lap_telemetry,
	telemetry_channels,
	laps,
	sessions,
	telemetry_sources
} from '../src/lib/server/db/schema';
import { eq, asc, inArray, getTableColumns } from 'drizzle-orm';
import { writeSessionTelemetry } from '../src/lib/server/telemetry/parquet';
import {
	convertDbTelemetryToParquetFormat,
	getSessionsNeedingMigration,
	validateMigration
} from '../src/lib/server/telemetry/migration';
import { existsSync } from 'fs';
import { join } from 'path';

const TELEMETRY_DIR = './data/telemetry';

interface MigrationOptions {
	dryRun: boolean;
	batchSize: number;
	sessionId?: number; // Optional: migrate specific session only
	skipValidation: boolean;
}

async function migrateSession(
	sessionId: number,
	options: MigrationOptions
): Promise<{ success: boolean; error?: string }> {
	try {
		// 1. Check if already has Parquet
		const parquetPath = join(TELEMETRY_DIR, `${sessionId}.parquet`);
		if (existsSync(parquetPath)) {
			console.log(`  Session ${sessionId}: Already has Parquet file, skipping`);
			return { success: true };
		}

		// 2. Fetch DB telemetry for session
		const dbTelemetry = await db
			.select()
			.from(lap_telemetry)
			.where(eq(lap_telemetry.sessionId, sessionId))
			.orderBy(asc(lap_telemetry.lapNumber));

		if (dbTelemetry.length === 0) {
			console.log(`  Session ${sessionId}: No telemetry in DB, skipping`);
			return { success: true };
		}

		// 3. Fetch auxiliary channels
		const lapIds = dbTelemetry.map((t) => t.id).filter(Boolean);
		const auxChannels =
			lapIds.length > 0
				? await db
						.select()
						.from(telemetry_channels)
						.where(inArray(telemetry_channels.lapId, lapIds))
				: [];

		// 4. Get source metadata
		const [source] = await db
			.select()
			.from(telemetry_sources)
			.where(eq(telemetry_sources.sessionId, sessionId))
			.limit(1);

		if (options.dryRun) {
			console.log(
				`  [DRY-RUN] Would migrate session ${sessionId} (${dbTelemetry.length} laps, ${auxChannels.length} aux channels)`
			);
			return { success: true };
		}

		// 5. Convert to Parquet format
		const sessionTelemetry = convertDbTelemetryToParquetFormat(sessionId, dbTelemetry, auxChannels);

		// Add metadata from source
		sessionTelemetry.metadata.sourceType = source?.type || 'unknown';

		// 6. Write Parquet file
		await writeSessionTelemetry(sessionId, sessionTelemetry);

		// 7. Validate (unless skipped)
		if (!options.skipValidation) {
			const validation = await validateMigration(sessionId);
			if (!validation.success) {
				return {
					success: false,
					error: `Validation failed: DB rows ${validation.dbRowCount}, Parquet rows ${validation.parquetRowCount}`
				};
			}
		}

		console.log(`  ✓ Session ${sessionId}: Migrated (${dbTelemetry.length} laps)`);
		return { success: true };
	} catch (error) {
		return { success: false, error: String(error) };
	}
}

async function main() {
	const args = process.argv.slice(2);
	const options: MigrationOptions = {
		dryRun: args.includes('--dry-run'),
		skipValidation: args.includes('--skip-validation'),
		batchSize: parseInt(args.find((a) => a.startsWith('--batch='))?.split('=')[1] || '10'),
		sessionId: args.find((a) => a.startsWith('--session='))?.split('=')[1]
			? parseInt(args.find((a) => a.startsWith('--session='))!.split('=')[1])
			: undefined
	};

	console.log('=== Telemetry Migration to Parquet ===');
	console.log(`Options: ${JSON.stringify(options, null, 2)}\n`);

	// Get sessions to migrate
	const sessionsToMigrate = options.sessionId
		? [{ sessionId: options.sessionId, hasParquet: false, lapCount: 0, hasDbData: true }]
		: await getSessionsNeedingMigration();

	console.log(`Found ${sessionsToMigrate.length} sessions to process\n`);

	if (sessionsToMigrate.length === 0) {
		console.log('No sessions need migration. Exiting.');
		process.exit(0);
	}

	// Process in batches
	let successCount = 0;
	let failCount = 0;

	for (let i = 0; i < sessionsToMigrate.length; i += options.batchSize) {
		const batch = sessionsToMigrate.slice(i, i + options.batchSize);
		console.log(
			`\nBatch ${Math.floor(i / options.batchSize) + 1}/${Math.ceil(sessionsToMigrate.length / options.batchSize)}`
		);

		for (const session of batch) {
			const result = await migrateSession(session.sessionId, options);
			if (result.success) {
				successCount++;
			} else {
				failCount++;
				console.error(`  ✗ Session ${session.sessionId}: ${result.error}`);
			}
		}
	}

	console.log(`\n=== Migration Complete ===`);
	console.log(`Success: ${successCount}`);
	console.log(`Failed: ${failCount}`);
	console.log(`Total: ${sessionsToMigrate.length}`);

	process.exit(failCount > 0 ? 1 : 0);
}

main().catch(console.error);
```

**CLI Usage:**

```bash
# Dry run (show what would be migrated)
npx tsx scripts/migrate-to-parquet.ts --dry-run

# Migrate all sessions
npx tsx scripts/migrate-to-parquet.ts

# Migrate specific session
npx tsx scripts/migrate-to-parquet.ts --session=123

# Skip validation for speed
npx tsx scripts/migrate-to-parquet.ts --skip-validation

# Custom batch size
npx tsx scripts/migrate-to-parquet.ts --batch=5
```

**Verify:**

```bash
# Check script compiles
npx tsc --noEmit scripts/migrate-to-parquet.ts 2>&1 | head -10
```

**Done:**

- [ ] Migration script created with all features
- [ ] Script has CLI argument parsing
- [ ] Dry-run mode implemented
- [ ] Progress reporting during batch processing
- [ ] Error handling continues on individual failures

---

## Task 3: Add Migration Script to package.json

**Type:** auto

**Files:** `package.json`

**Action:**
Add the migration script as an npm script for easy execution.

**package.json addition:**

```json
{
	"scripts": {
		"telemetry:migrate": "tsx scripts/migrate-to-parquet.ts",
		"telemetry:migrate:dry-run": "tsx scripts/migrate-to-parquet.ts --dry-run",
		"telemetry:migrate:validate": "tsx scripts/migrate-to-parquet.ts --skip-validation"
	}
}
```

**Verify:**

```bash
# Check scripts are valid JSON
cat package.json | npx json scripts.telemetry:migrate
```

**Done:**

- [ ] `telemetry:migrate` script added
- [ ] `telemetry:migrate:dry-run` script added
- [ ] `telemetry:migrate:validate` script added

---

## Task 4: Run Migration and Verify Compression

**Type:** checkpoint:manual

**Action:**
Run the migration script on the actual database and verify compression targets are met.

**Steps:**

1. Run dry-run first to see what will be migrated:

   ```bash
   npm run telemetry:migrate:dry-run
   ```

2. Run actual migration:

   ```bash
   npm run telemetry:migrate
   ```

3. Measure compression:

   ```bash
   # Get size of DB telemetry data
   npx drizzle-kit studio &
   # Or query directly to estimate

   # Get size of Parquet files
   du -sh data/telemetry/

   # Compare sizes
   echo "=== Size Comparison ==="
   echo "Parquet files:"
   ls -lh data/telemetry/*.parquet | awk '{sum+=$5} END {print "Total: " sum}'
   ```

**Compression Target:**

- Parquet files should be 50%+ smaller than equivalent JSON storage
- If not met, investigate column encoding (should use dictionary encoding for gear, binary for floating point)

**Verify:**

```bash
# Check Parquet files were created
ls -la data/telemetry/*.parquet | wc -l

# Verify file sizes are reasonable
ls -lh data/telemetry/*.parquet
```

**Done:**

- [ ] Migration script executed successfully
- [ ] All existing sessions have Parquet files
- [ ] Compression target (50%+) verified
- [ ] Validation passed (row counts match)

---

## Task 5: Create Cleanup Script (Optional/Deferred)

**Type:** auto

**Files:** `scripts/cleanup-db-telemetry.ts`

**Action:**
Create a script to truncate the old `lap_telemetry` and `telemetry_channels` data after successful migration. **Run this only after confirming Parquet files work correctly in production.**

**Script Features:**

1. Verify all sessions have valid Parquet files before cleanup
2. Archive option: Export old data to backup file before deletion
3. Dry-run mode
4. Selective cleanup (specific sessions or all)

**Implementation (skeleton):**

```typescript
#!/usr/bin/env tsx
// scripts/cleanup-db-telemetry.ts
// WARNING: Destructive operation - only run after verifying Parquet migration

import { db } from '../src/lib/server/db/client';
import { lap_telemetry, telemetry_channels } from '../src/lib/server/db/schema';
import { existsSync } from 'fs';
import { join } from 'path';

const TELEMETRY_DIR = './data/telemetry';

async function main() {
	const args = process.argv.slice(2);
	const dryRun = args.includes('--dry-run');
	const force = args.includes('--force');

	if (!force && !dryRun) {
		console.log('WARNING: This will delete telemetry data from the database.');
		console.log('Parquet files must exist and be verified first.');
		console.log('Use --force to proceed or --dry-run to preview.');
		process.exit(1);
	}

	// Verify all sessions have Parquet files...
	// Truncate tables if verification passes...
}

main().catch(console.error);
```

**Note:** This script should be run manually and cautiously after thorough testing.

**Verify:**

```bash
npx tsc --noEmit scripts/cleanup-db-telemetry.ts 2>&1 | head -10
```

**Done:**

- [ ] Cleanup script created
- [ ] Script has safety checks (--force required)
- [ ] Dry-run mode implemented
- [ ] Verification that Parquet files exist before cleanup

---

## Verification Criteria

1. **Migration Success**: All existing sessions migrated without errors
2. **Data Integrity**: Row counts match between DB and Parquet
3. **Compression**: 50%+ size reduction achieved
4. **Script Quality**: Migration script has proper CLI interface and error handling
5. **Safety**: Cleanup script requires explicit flags to prevent accidental runs

---

## Success Criteria

- [ ] Migration script successfully converts all existing sessions
- [ ] Parquet files are 50%+ smaller than equivalent JSON storage
- [ ] Data validation confirms all rows migrated correctly
- [ ] npm scripts added for easy migration execution
- [ ] Cleanup script created (for future use after verification)
- [ ] No data loss during migration process
